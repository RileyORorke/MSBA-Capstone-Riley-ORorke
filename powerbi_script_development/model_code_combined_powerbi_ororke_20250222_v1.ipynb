{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 10\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjoblib\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Load dataset from Power BI (or from SQL if used in Power BI)\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m  \u001b[38;5;66;03m# Power BI automatically loads data\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Drop unnecessary columns\u001b[39;00m\n\u001b[0;32m     13\u001b[0m columns_to_drop \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD.[NamePostfix]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF.[Name]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mG1.[ParameterID]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mG1.[Name]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mG1.[OperatorMessage]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[0;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mG2.[ParameterID]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mG2.[Name]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mG2.[Description]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mG3.[ParameterID]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mG3.[Name]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB.[Description]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC.[ParameterID]\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     19\u001b[0m ]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# 'dataset' holds the input data for this script\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Load dataset from Power BI\n",
    "df = dataset  \n",
    "\n",
    "# Drop unnecessary columns\n",
    "columns_to_drop = [\n",
    "    \"D.[NamePostfix]\", \"F.[Name]\", \"G1.[ParameterID]\", \"G1.[Name]\", \"G1.[OperatorMessage]\", \n",
    "    \"G2.[ParameterID]\", \"G2.[Name]\", \"G2.[Description]\", \"G3.[ParameterID]\", \"G3.[Name]\", \n",
    "    \"G3.[OperatorMessage]\", \"A.[ParameterID]\", \"A.[EntryTimestamp]\", \"A.[DataValue]\", \n",
    "    \"A.[Description]\", \"B.[ParameterID]\", \"B.[EntryTimestamp]\", \"B.[DataValue]\", \n",
    "    \"B.[Description]\", \"C.[ParameterID]\"\n",
    "]\n",
    "df = df.drop(columns=[col for col in columns_to_drop if col in df.columns], errors='ignore')\n",
    "\n",
    "# Convert measurement values to numeric\n",
    "df['C.[DataValue]'] = pd.to_numeric(df['C.[DataValue]'], errors='coerce')\n",
    "\n",
    "# Convert timestamps to datetime and sort\n",
    "df['C.[EntryTimestamp]'] = pd.to_datetime(df['C.[EntryTimestamp]'], errors='coerce')\n",
    "df = df.dropna(subset=['C.[EntryTimestamp]']).sort_values(by='C.[EntryTimestamp]').reset_index(drop=True)\n",
    "\n",
    "# Outlier Removal Using IQR\n",
    "Q1 = df[\"C.[DataValue]\"].quantile(0.25)\n",
    "Q3 = df[\"C.[DataValue]\"].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound, upper_bound = Q1 - 1.5 * IQR, Q3 + 1.5 * IQR\n",
    "df = df[(df[\"C.[DataValue]\"] >= lower_bound) & (df[\"C.[DataValue]\"] <= upper_bound)].reset_index(drop=True)\n",
    "\n",
    "# Create Lag Features (Updated: Removed DataValue_Lag1)\n",
    "for lag in [3, 5]:\n",
    "    df[f\"DataValue_Lag{lag}\"] = df[\"C.[DataValue]\"].shift(lag)\n",
    "\n",
    "# Create Rolling Statistics (Matching Updated Features)\n",
    "df[\"Rolling_Mean_3\"] = df[\"C.[DataValue]\"].rolling(window=3).mean()\n",
    "df[\"Rolling_Std_3\"] = df[\"C.[DataValue]\"].rolling(window=3).std()\n",
    "df[\"Rolling_Mean_5\"] = df[\"C.[DataValue]\"].rolling(window=5).mean()\n",
    "df[\"Rolling_Std_5\"] = df[\"C.[DataValue]\"].rolling(window=5).std()\n",
    "\n",
    "# Drop rows with NaN values (due to shifting)\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "# Create a Cycle Count\n",
    "df[\"Cycle_Count\"] = range(1, len(df) + 1)\n",
    "\n",
    "# Define feature columns (Updated to match new model)\n",
    "feature_columns = [\n",
    "    \"Cycle_Count\", \"C.[DataValue]\", \"DataValue_Lag3\", \"DataValue_Lag5\",\n",
    "    \"Rolling_Mean_3\", \"Rolling_Std_3\", \"Rolling_Mean_5\", \"Rolling_Std_5\"\n",
    "]\n",
    "\n",
    "# Define target variable and drop last row to avoid NaN target\n",
    "df[\"Target_NextCycle\"] = df[\"C.[DataValue]\"].shift(-1)\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "# Train a Random Forest Regressor (Updated Hyperparameters)\n",
    "X = df[feature_columns]\n",
    "y = df[\"Target_NextCycle\"]\n",
    "model = RandomForestRegressor(\n",
    "    n_estimators=40,        \n",
    "    max_depth=5,            \n",
    "    min_samples_split=15,    \n",
    "    min_samples_leaf=7,       \n",
    "    max_features=\"sqrt\",     \n",
    "    bootstrap=True,          \n",
    "    random_state=42\n",
    ")\n",
    "model.fit(X, y)\n",
    "\n",
    "# Define how many future cycles to predict\n",
    "future_cycles = 10  # Adjust as needed\n",
    "\n",
    "# Create a DataFrame for future predictions\n",
    "future_df = pd.DataFrame()\n",
    "future_df[\"Cycle_Count\"] = range(df[\"Cycle_Count\"].max() + 1, df[\"Cycle_Count\"].max() + 1 + future_cycles)\n",
    "\n",
    "# Use the last known values as the starting point for predictions\n",
    "last_known_values = df.iloc[-1][feature_columns].to_dict()\n",
    "predicted_bores = []\n",
    "\n",
    "# Predict future bore sizes using the trained regression model\n",
    "for cycle in future_df[\"Cycle_Count\"]:\n",
    "    new_row = last_known_values.copy()\n",
    "    new_row[\"Cycle_Count\"] = cycle\n",
    "\n",
    "    # Shift lag values forward\n",
    "    for lag in [3, 5]:  # Updated to match the correct lag features\n",
    "        new_row[f\"DataValue_Lag{lag}\"] = predicted_bores[-lag] if len(predicted_bores) >= lag else last_known_values[\"C.[DataValue]\"]\n",
    "\n",
    "    # Update rolling statistics dynamically\n",
    "    new_row[\"Rolling_Mean_3\"] = np.mean(predicted_bores[-3:]) if len(predicted_bores) >= 3 else last_known_values[\"Rolling_Mean_3\"]\n",
    "    new_row[\"Rolling_Std_3\"] = np.std(predicted_bores[-3:]) if len(predicted_bores) >= 3 else last_known_values[\"Rolling_Std_3\"]\n",
    "    new_row[\"Rolling_Mean_5\"] = np.mean(predicted_bores[-5:]) if len(predicted_bores) >= 5 else last_known_values[\"Rolling_Mean_5\"]\n",
    "    new_row[\"Rolling_Std_5\"] = np.std(predicted_bores[-5:]) if len(predicted_bores) >= 5 else last_known_values[\"Rolling_Std_5\"]\n",
    "\n",
    "    # Convert to DataFrame and predict bore size\n",
    "    new_X = pd.DataFrame([new_row])[feature_columns]\n",
    "    predicted_bore = model.predict(new_X)[0]\n",
    "    predicted_bores.append(predicted_bore)\n",
    "\n",
    "    # Store new row values for further processing\n",
    "    future_df.loc[future_df[\"Cycle_Count\"] == cycle, \"Predicted_Bore_Size\"] = predicted_bore\n",
    "\n",
    "# Compute bore size changes over time\n",
    "future_df[\"Bore_Size_Change\"] = future_df[\"Predicted_Bore_Size\"].diff().fillna(0)\n",
    "\n",
    "# Define wear classification function\n",
    "def classify_wear(change):\n",
    "    if change < 0.001:\n",
    "        return \"Normal Wear\"\n",
    "    elif 0.001 <= change < 0.005:\n",
    "        return \"Moderate Wear\"\n",
    "    else:\n",
    "        return \"Critical Wear\"\n",
    "\n",
    "# Assign wear labels to future cycles\n",
    "future_df[\"Predicted_Wear_Stage\"] = future_df[\"Bore_Size_Change\"].apply(classify_wear)\n",
    "\n",
    "# Combine actual & future data\n",
    "df[\"Predicted_Bore_Size\"] = np.nan  # Set actual cycles to NaN in prediction column\n",
    "df[\"Predicted_Wear_Stage\"] = np.nan\n",
    "\n",
    "# Final dataset\n",
    "final_df = pd.concat([df, future_df], ignore_index=True)\n",
    "\n",
    "final_df\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
