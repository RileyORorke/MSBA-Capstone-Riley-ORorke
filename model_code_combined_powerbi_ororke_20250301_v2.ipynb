{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 10\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjoblib\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Load dataset from Power BI (or from SQL if used in Power BI)\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m  \u001b[38;5;66;03m# Power BI automatically loads data\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Drop unnecessary columns\u001b[39;00m\n\u001b[0;32m     13\u001b[0m columns_to_drop \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD.[NamePostfix]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF.[Name]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mG1.[ParameterID]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mG1.[Name]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mG1.[OperatorMessage]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[0;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mG2.[ParameterID]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mG2.[Name]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mG2.[Description]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mG3.[ParameterID]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mG3.[Name]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB.[Description]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC.[ParameterID]\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     19\u001b[0m ]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "# Load dataset from Power BI (or from SQL if used in Power BI)\n",
    "df = dataset  # Power BI automatically loads data\n",
    "\n",
    "\n",
    "def drop_unnecessary_columns(df):\n",
    "    \"\"\"Removes unnecessary columns from the dataset if they exist.\"\"\"\n",
    "    columns_to_drop = [\n",
    "        \"D.[NamePostfix]\", \"F.[Name]\", \"G1.[ParameterID]\", \"G1.[Name]\", \"G1.[OperatorMessage]\",\n",
    "        \"G2.[ParameterID]\", \"G2.[Name]\", \"G2.[Description]\", \"G3.[ParameterID]\", \"G3.[Name]\",\n",
    "        \"G3.[OperatorMessage]\", \"A.[ParameterID]\", \"A.[EntryTimestamp]\", \"A.[DataValue]\",\n",
    "        \"A.[Description]\", \"B.[ParameterID]\", \"B.[EntryTimestamp]\", \"B.[DataValue]\",\n",
    "        \"B.[Description]\", \"C.[ParameterID]\"\n",
    "    ]\n",
    "    return df.drop(columns=[col for col in columns_to_drop if col in df.columns], errors=\"ignore\")\n",
    "\n",
    "\n",
    "def remove_outliers(df, column):\n",
    "    \"\"\"Removes outliers using the Interquartile Range (IQR) method.\"\"\"\n",
    "    q1 = df[column].quantile(0.25)\n",
    "    q3 = df[column].quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    lower_bound = q1 - 1.5 * iqr\n",
    "    upper_bound = q3 + 1.5 * iqr\n",
    "    return df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]\n",
    "\n",
    "\n",
    "# Data Cleaning and Preprocessing\n",
    "df = drop_unnecessary_columns(df)\n",
    "df[\"C.[DataValue]\"] = pd.to_numeric(df[\"C.[DataValue]\"], errors=\"coerce\")\n",
    "df[\"C.[EntryTimestamp]\"] = pd.to_datetime(df[\"C.[EntryTimestamp]\"], errors=\"coerce\")\n",
    "df = df.dropna(subset=[\"C.[EntryTimestamp]\"]).sort_values(by=\"C.[EntryTimestamp]\").reset_index(drop=True)\n",
    "df = remove_outliers(df, \"C.[DataValue]\").reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Feature Engineering\n",
    "def create_lag_features(df, column, lags):\n",
    "    \"\"\"Generates lag features for the specified column.\"\"\"\n",
    "    for lag in lags:\n",
    "        df[f\"{column}_lag{lag}\"] = df[column].shift(lag)\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_rolling_features(df, column, windows):\n",
    "    \"\"\"Generates rolling mean and standard deviation features.\"\"\"\n",
    "    for window in windows:\n",
    "        df[f\"{column}_rolling_mean_{window}\"] = df[column].rolling(window=window).mean()\n",
    "        df[f\"{column}_rolling_std_{window}\"] = df[column].rolling(window=window).std()\n",
    "    return df\n",
    "\n",
    "\n",
    "# Apply feature engineering\n",
    "df = create_lag_features(df, \"C.[DataValue]\", lags=[1, 2, 3, 5])\n",
    "df = create_rolling_features(df, \"C.[DataValue]\", windows=[3, 5])\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "df[\"cycle_count\"] = range(1, len(df) + 1)\n",
    "\n",
    "\n",
    "# Define feature columns\n",
    "feature_columns = [\n",
    "    \"cycle_count\", \"C.[DataValue]\", \"C.[DataValue]_lag1\", \"C.[DataValue]_lag2\",\n",
    "    \"C.[DataValue]_lag3\", \"C.[DataValue]_lag5\", \"C.[DataValue]_rolling_mean_3\",\n",
    "    \"C.[DataValue]_rolling_std_3\", \"C.[DataValue]_rolling_mean_5\", \"C.[DataValue]_rolling_std_5\"\n",
    "]\n",
    "\n",
    "# Define target variable\n",
    "df[\"target_next_cycle\"] = df[\"C.[DataValue]\"].shift(-1)\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "# Train a Random Forest Regressor\n",
    "X = df[feature_columns]\n",
    "y = df[\"target_next_cycle\"]\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X, y)\n",
    "\n",
    "\n",
    "def generate_future_predictions(model, df, feature_columns, future_cycles=10):\n",
    "    \"\"\"\n",
    "    Predicts bore sizes for future cycles using the trained model.\n",
    "    \"\"\"\n",
    "    future_df = pd.DataFrame()\n",
    "    future_df[\"cycle_count\"] = range(df[\"cycle_count\"].max() + 1, df[\"cycle_count\"].max() + 1 + future_cycles)\n",
    "\n",
    "    last_known_values = df.iloc[-1][feature_columns].to_dict()\n",
    "    predicted_bores = []\n",
    "\n",
    "    for cycle in future_df[\"cycle_count\"]:\n",
    "        new_row = last_known_values.copy()\n",
    "        new_row[\"cycle_count\"] = cycle\n",
    "\n",
    "        for lag in [1, 2, 3, 5]:\n",
    "            new_row[f\"C.[DataValue]_lag{lag}\"] = (\n",
    "                predicted_bores[-lag] if len(predicted_bores) >= lag else last_known_values[\"C.[DataValue]\"]\n",
    "            )\n",
    "\n",
    "        for window in [3, 5]:\n",
    "            new_row[f\"C.[DataValue]_rolling_mean_{window}\"] = (\n",
    "                np.mean(predicted_bores[-window:]) if len(predicted_bores) >= window else last_known_values[f\"C.[DataValue]_rolling_mean_{window}\"]\n",
    "            )\n",
    "            new_row[f\"C.[DataValue]_rolling_std_{window}\"] = (\n",
    "                np.std(predicted_bores[-window:]) if len(predicted_bores) >= window else last_known_values[f\"C.[DataValue]_rolling_std_{window}\"]\n",
    "            )\n",
    "\n",
    "        new_X = pd.DataFrame([new_row])[feature_columns]\n",
    "        predicted_bore = model.predict(new_X)[0]\n",
    "        predicted_bores.append(predicted_bore)\n",
    "\n",
    "        future_df.loc[future_df[\"cycle_count\"] == cycle, \"predicted_bore_size\"] = predicted_bore\n",
    "\n",
    "    future_df[\"bore_size_change\"] = future_df[\"predicted_bore_size\"].diff().fillna(0)\n",
    "\n",
    "    return future_df\n",
    "\n",
    "\n",
    "def classify_wear(change):\n",
    "    \"\"\"Classifies wear severity based on bore size change.\"\"\"\n",
    "    if change < 0.001:\n",
    "        return \"Normal Wear\"\n",
    "    elif 0.001 <= change < 0.005:\n",
    "        return \"Moderate Wear\"\n",
    "    return \"Critical Wear\"\n",
    "\n",
    "\n",
    "# Generate predictions and classify wear stages\n",
    "future_cycles = 10\n",
    "future_df = generate_future_predictions(model, df, feature_columns, future_cycles)\n",
    "future_df[\"predicted_wear_stage\"] = future_df[\"bore_size_change\"].apply(classify_wear)\n",
    "\n",
    "# Combine actual & future data\n",
    "df[\"predicted_bore_size\"] = np.nan\n",
    "df[\"predicted_wear_stage\"] = np.nan\n",
    "final_df = pd.concat([df, future_df], ignore_index=True)\n",
    "\n",
    "final_df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
